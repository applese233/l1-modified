2025-08-13 11:45:29,793 INFO    MainThread:130376 [wandb_setup.py:_flush():80] Current SDK version is 0.21.1
2025-08-13 11:45:29,793 INFO    MainThread:130376 [wandb_setup.py:_flush():80] Configure stats pid to 130376
2025-08-13 11:45:29,793 INFO    MainThread:130376 [wandb_setup.py:_flush():80] Loading settings from /home/yiran/.config/wandb/settings
2025-08-13 11:45:29,793 INFO    MainThread:130376 [wandb_setup.py:_flush():80] Loading settings from /home/yiran/yaoqi_workplace/l1/wandb/settings
2025-08-13 11:45:29,793 INFO    MainThread:130376 [wandb_setup.py:_flush():80] Loading settings from environment variables
2025-08-13 11:45:29,793 INFO    MainThread:130376 [wandb_init.py:setup_run_log_directory():703] Logging user logs to /home/yiran/yaoqi_workplace/l1/wandb/run-20250813_114529-fh0l9amm/logs/debug.log
2025-08-13 11:45:29,794 INFO    MainThread:130376 [wandb_init.py:setup_run_log_directory():704] Logging internal logs to /home/yiran/yaoqi_workplace/l1/wandb/run-20250813_114529-fh0l9amm/logs/debug-internal.log
2025-08-13 11:45:29,794 INFO    MainThread:130376 [wandb_init.py:init():830] calling init triggers
2025-08-13 11:45:29,794 INFO    MainThread:130376 [wandb_init.py:init():835] wandb.init called with sweep_config: {}
config: {'data': {'sampler': {'class_path': None, 'class_name': None}, 'dataloader_num_workers': 4, 'tokenizer': None, 'train_files': '/disk3/yiran/yaoqi/l1/data_exact/train.parquet', 'val_files': '/disk3/yiran/yaoqi/l1/data_exact/aime.parquet', 'prompt_key': 'prompt', 'reward_fn_key': 'data_source', 'max_prompt_length': 512, 'max_response_length': 2048, 'train_batch_size': 8, 'val_batch_size': 8, 'return_raw_input_ids': False, 'return_raw_chat': False, 'shuffle': True, 'filter_overlong_prompts': True, 'filter_overlong_prompts_workers': 1, 'truncation': 'error', 'image_key': 'images'}, 'actor_rollout_ref': {'hybrid_engine': True, 'model': {'path': 'Qwen/Qwen2.5-1.5B-Instruct', 'external_lib': None, 'override_config': {}, 'enable_gradient_checkpointing': True, 'use_remove_padding': True, 'dtype': 'bfloat16'}, 'actor': {'_target_': 'verl.workers.config.FSDPActorConfig', 'strategy': 'fsdp', 'ppo_mini_batch_size': 8, 'ppo_micro_batch_size': None, 'ppo_micro_batch_size_per_gpu': None, 'use_dynamic_bsz': True, 'ppo_max_token_len_per_gpu': 16384, 'grad_clip': 1.0, 'clip_ratio': 0.2, 'clip_ratio_low': 0.2, 'clip_ratio_high': 0.2, 'clip_ratio_c': 3.0, 'loss_agg_mode': 'token-mean', 'entropy_coeff': 0.001, 'use_kl_loss': True, 'use_torch_compile': True, 'kl_loss_coef': 0.001, 'kl_loss_type': 'low_var_kl', 'ppo_epochs': 1, 'shuffle': False, 'ulysses_sequence_parallel_size': 1, 'checkpoint': {'contents': ['model', 'optimizer', 'extra']}, 'optim': {'_target_': 'verl.workers.config.FSDPOptimizerConfig', 'lr': 1e-06, 'lr_warmup_steps': -1, 'lr_warmup_steps_ratio': 0.0, 'min_lr_ratio': None, 'warmup_style': 'constant', 'total_training_steps': 15060, 'weight_decay': 0.01}, 'fsdp_config': {'_target_': 'verl.workers.config.FSDPEngineConfig', 'wrap_policy': {'min_num_params': 0}, 'param_offload': False, 'optimizer_offload': False, 'fsdp_size': -1, 'model_dtype': 'bfloat16'}}, 'ref': {'fsdp_config': {'_target_': 'verl.workers.config.FSDPEngineConfig', 'param_offload': True, 'wrap_policy': {'min_num_params': 0}, 'model_dtype': 'bfloat16'}, 'entropy_from_logits_with_chunking': False, 'log_prob_micro_batch_size': None, 'log_prob_micro_batch_size_per_gpu': None, 'log_prob_use_dynamic_bsz': True, 'log_prob_max_token_len_per_gpu': 16384, 'ulysses_sequence_parallel_size': 1}, 'rollout': {'_target_': 'verl.workers.config.RolloutConfig', 'name': 'vllm', 'temperature': 0.6, 'top_k': -1, 'top_p': 1, 'prompt_length': 512, 'response_length': 2048, 'dtype': 'bfloat16', 'gpu_memory_utilization': 0.3, 'ignore_eos': False, 'enforce_eager': False, 'free_cache_engine': False, 'load_format': 'safetensors', 'tensor_model_parallel_size': 1, 'max_num_batched_tokens': 32768, 'max_model_len': None, 'max_num_seqs': 1024, 'log_prob_micro_batch_size': None, 'log_prob_micro_batch_size_per_gpu': None, 'log_prob_use_dynamic_bsz': True, 'log_prob_max_token_len_per_gpu': 16384, 'disable_log_stats': True, 'enable_chunked_prefill': True, 'do_sample': True, 'multi_turn': {'enable': False}, 'mode': 'sync', 'calculate_log_probs': True, 'n': 4, 'val_kwargs': {'_target_': 'verl.workers.config.SamplingConfig', 'top_k': -1, 'top_p': 1.0, 'temperature': 0.6, 'n': 4, 'do_sample': False}}}, 'critic': {'rollout_n': 4, 'strategy': 'fsdp', 'optim': {'lr': 1e-05, 'lr_warmup_steps_ratio': 0.0, 'min_lr_ratio': None, 'warmup_style': 'constant', 'total_training_steps': 15060, 'weight_decay': 0.01}, 'model': {'path': '~/models/deepseek-llm-7b-chat', 'tokenizer_path': 'Qwen/Qwen2.5-1.5B-Instruct', 'override_config': {}, 'external_lib': None, 'enable_gradient_checkpointing': True, 'use_remove_padding': False, 'dtype': 'bfloat16', 'fsdp_config': {'param_offload': False, 'optimizer_offload': False, 'wrap_policy': {'min_num_params': 0}, 'fsdp_size': -1}}, 'ppo_mini_batch_size': 8, 'ppo_micro_batch_size': None, 'ppo_micro_batch_size_per_gpu': None, 'forward_micro_batch_size': None, 'forward_micro_batch_size_per_gpu': None, 'use_dynamic_bsz': True, 'ppo_max_token_len_per_gpu': 32768, 'forward_max_token_len_per_gpu': 32768, 'ulysses_sequence_parallel_size': 1, 'ppo_epochs': 1, 'shuffle': False, 'grad_clip': 1.0, 'cliprange_value': 0.5, 'enable': False, 'checkpoint': {'contents': ['model', 'optimizer', 'extra']}}, 'reward_model': {'enable': False, 'strategy': 'fsdp', 'model': {'input_tokenizer': 'Qwen/Qwen2.5-1.5B-Instruct', 'path': '~/models/FsfairX-LLaMA3-RM-v0.1', 'external_lib': None, 'use_remove_padding': False, 'fsdp_config': {'wrap_policy': {'min_num_params': 0}, 'param_offload': False, 'fsdp_size': -1}}, 'micro_batch_size': None, 'micro_batch_size_per_gpu': None, 'max_length': None, 'ulysses_sequence_parallel_size': 1, 'use_dynamic_bsz': True, 'forward_max_token_len_per_gpu': 32768, 'reward_manager': 'naive', 'launch_reward_fn_async': False}, 'custom_reward_function': {'path': None, 'name': 'compute_score'}, 'algorithm': {'_target_': 'verl.trainer.config.AlgoConfig', 'gamma': 1.0, 'lam': 1.0, 'adv_estimator': 'grpo', 'use_kl_in_reward': False, 'kl_penalty': 'kl', 'kl_ctrl': {'type': 'fixed', 'kl_coef': 0.001, 'horizon': 10000, 'target_kl': 0.1}}, 'trainer': {'device': 'cuda', 'balance_batch': True, 'total_epochs': 3, 'total_training_steps': None, 'project_name': 'l1finetune', 'experiment_name': 'qwen2.5-1.5B-Instruct-l1_exact', 'logger': ['console', 'wandb'], 'log_val_generations': 0, 'nnodes': 1, 'n_gpus_per_node': 8, 'save_freq': 5000, 'resume_mode': 'auto', 'resume_from_path': None, 'val_before_train': True, 'test_freq': 100, 'critic_warmup': 0, 'default_hdfs_dir': None, 'del_local_ckpt_after_load': False, 'default_local_dir': '/disk3/yiran/yaoqi/l1/checkpoints/l1finetune/qwen2.5-1.5B-Instruct-l1_exact', 'max_actor_ckpt_to_keep': None, 'max_critic_ckpt_to_keep': None, 'esi_redundant_time': 0}, 'reward_config': {'alpha': 0.0003, 'sigmoid_reward': False, 'linear_reward': True, 'multiplier_reward': False}, 'global_profiler': {'_target_': 'verl.utils.profiler.ProfilerConfig', 'tool': None, 'steps': None, 'profile_continuous_steps': False, 'save_path': 'outputs/profile', 'global_tool_config': {'nsys': {'_target_': 'verl.utils.profiler.config.NsightToolConfig', 'discrete': False, 'controller_nsight_options': {'trace': 'cuda,nvtx,cublas,ucx', 'cuda-memory-usage': 'true', 'cuda-graph-trace': 'graph'}, 'worker_nsight_options': {'trace': 'cuda,nvtx,cublas,ucx', 'cuda-memory-usage': 'true', 'cuda-graph-trace': 'graph', 'capture-range': 'cudaProfilerApi', 'capture-range-end': None, 'kill': 'none'}}}}, 'ray_init': {'num_cpus': None, 'timeline_json_file': None}, '_wandb': {}}
2025-08-13 11:45:29,794 INFO    MainThread:130376 [wandb_init.py:init():871] starting backend
2025-08-13 11:45:29,999 INFO    MainThread:130376 [wandb_init.py:init():874] sending inform_init request
2025-08-13 11:45:30,002 INFO    MainThread:130376 [wandb_init.py:init():882] backend started and connected
2025-08-13 11:45:30,004 INFO    MainThread:130376 [wandb_init.py:init():953] updated telemetry
2025-08-13 11:45:30,008 INFO    MainThread:130376 [wandb_init.py:init():977] communicating run to backend with 90.0 second timeout
2025-08-13 11:45:30,948 INFO    MainThread:130376 [wandb_init.py:init():1029] starting run threads in backend
2025-08-13 11:45:31,068 INFO    MainThread:130376 [wandb_run.py:_console_start():2494] atexit reg
2025-08-13 11:45:31,069 INFO    MainThread:130376 [wandb_run.py:_redirect():2342] redirect: wrap_raw
2025-08-13 11:45:31,069 INFO    MainThread:130376 [wandb_run.py:_redirect():2411] Wrapping output streams.
2025-08-13 11:45:31,069 INFO    MainThread:130376 [wandb_run.py:_redirect():2434] Redirects installed.
2025-08-13 11:45:31,070 INFO    MainThread:130376 [wandb_init.py:init():1075] run started, returning control to user process
