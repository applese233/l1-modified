Checkpoint tracker file does not exist: /disk3/yiran/yaoqi/l1/checkpoints/l1finetune/qwen2.5-1.5B-Instruct-l1_exact/latest_checkpointed_iteration.txt
Training from scratch
test_gen_batch meta info: {'eos_token_id': 151645, 'pad_token_id': 151643, 'recompute_log_prob': False, 'do_sample': False, 'validate': True, 'global_steps': 0}
validation generation end
delta_score: -0.10739999999999994, reward_response.is_correct: False, num_tokens: 932, valid_response_length: 574delta_score: -0.10739999999999994, reward_response.is_correct: False, num_tokens: 932, valid_response_length: 574delta_score: -0.9995999999999999, reward_response.is_correct: False, num_tokens: 3890, valid_response_length: 558
delta_score: -0.7298999999999999, reward_response.is_correct: False, num_tokens: 2762, valid_response_length: 329delta_score: -0.9995999999999999, reward_response.is_correct: False, num_tokens: 3890, valid_response_length: 558delta_score: -0.7757999999999999, reward_response.is_correct: False, num_tokens: 3464, valid_response_length: 878delta_score: -0.7757999999999999, reward_response.is_correct: False, num_tokens: 3464, valid_response_length: 878
delta_score: -0.20219999999999994, reward_response.is_correct: False, num_tokens: 332, valid_response_length: 1006



delta_score: -0.22829999999999995, reward_response.is_correct: False, num_tokens: 3335, valid_response_length: 4096
delta_score: -0.22829999999999995, reward_response.is_correct: False, num_tokens: 3335, valid_response_length: 4096delta_score: -0.08550000000000002, reward_response.is_correct: False, num_tokens: 689, valid_response_length: 404delta_score: -0.4314, reward_response.is_correct: False, num_tokens: 2658, valid_response_length: 4096
delta_score: -0.20219999999999994, reward_response.is_correct: False, num_tokens: 332, valid_response_length: 1006delta_score: -0.08550000000000002, reward_response.is_correct: False, num_tokens: 689, valid_response_length: 404delta_score: -0.7298999999999999, reward_response.is_correct: False, num_tokens: 2762, valid_response_length: 329
delta_score: -0.4314, reward_response.is_correct: False, num_tokens: 2658, valid_response_length: 4096






reward_tensor's shape : torch.Size([16, 4096])
len reward_extra_infos_dict['reward']: 16
test_gen_batch meta info: {'eos_token_id': 151645, 'pad_token_id': 151643, 'recompute_log_prob': False, 'do_sample': False, 'validate': True, 'global_steps': 0}
validation generation end
delta_score: -0.7910999999999999, reward_response.is_correct: False, num_tokens: 3408, valid_response_length: 771delta_score: -0.7910999999999999, reward_response.is_correct: False, num_tokens: 3408, valid_response_length: 771delta_score: -0.8666999999999999, reward_response.is_correct: False, num_tokens: 3146, valid_response_length: 257delta_score: -0.8726999999999999, reward_response.is_correct: False, num_tokens: 3692, valid_response_length: 783delta_score: -0.15239999999999998, reward_response.is_correct: False, num_tokens: 1143, valid_response_length: 635delta_score: -0.15239999999999998, reward_response.is_correct: False, num_tokens: 1143, valid_response_length: 635
delta_score: -0.7580999999999999, reward_response.is_correct: False, num_tokens: 3374, valid_response_length: 847
delta_score: -0.20489999999999997, reward_response.is_correct: False, num_tokens: 1155, valid_response_length: 472delta_score: -0.7580999999999999, reward_response.is_correct: False, num_tokens: 3374, valid_response_length: 847
delta_score: -0.8666999999999999, reward_response.is_correct: False, num_tokens: 3146, valid_response_length: 257delta_score: -0.609, reward_response.is_correct: False, num_tokens: 2436, valid_response_length: 406delta_score: -0.8402999999999999, reward_response.is_correct: False, num_tokens: 1295, valid_response_length: 4096delta_score: -0.609, reward_response.is_correct: False, num_tokens: 2436, valid_response_length: 406

delta_score: -0.8726999999999999, reward_response.is_correct: False, num_tokens: 3692, valid_response_length: 783

delta_score: -0.8402999999999999, reward_response.is_correct: False, num_tokens: 1295, valid_response_length: 4096





delta_score: -0.20489999999999997, reward_response.is_correct: False, num_tokens: 1155, valid_response_length: 472


reward_tensor's shape : torch.Size([16, 4096])
len reward_extra_infos_dict['reward']: 32
test_gen_batch meta info: {'eos_token_id': 151645, 'pad_token_id': 151643, 'recompute_log_prob': False, 'do_sample': False, 'validate': True, 'global_steps': 0}
validation generation end
delta_score: -0.2541, reward_response.is_correct: False, num_tokens: 1970, valid_response_length: 1123delta_score: -0.0726, reward_response.is_correct: False, num_tokens: 3854, valid_response_length: 4096
delta_score: -0.2541, reward_response.is_correct: False, num_tokens: 1970, valid_response_length: 1123
delta_score: -0.6354, reward_response.is_correct: False, num_tokens: 2740, valid_response_length: 622delta_score: -0.6354, reward_response.is_correct: False, num_tokens: 2740, valid_response_length: 622
delta_score: -0.0726, reward_response.is_correct: False, num_tokens: 3854, valid_response_length: 4096delta_score: -0.6398999999999999, reward_response.is_correct: False, num_tokens: 1963, valid_response_length: 4096

delta_score: -0.9332999999999999, reward_response.is_correct: False, num_tokens: 3860, valid_response_length: 749delta_score: -0.6398999999999999, reward_response.is_correct: False, num_tokens: 1963, valid_response_length: 4096delta_score: -0.5966999999999999, reward_response.is_correct: False, num_tokens: 2479, valid_response_length: 490delta_score: -0.5966999999999999, reward_response.is_correct: False, num_tokens: 2479, valid_response_length: 490
delta_score: -0.40259999999999996, reward_response.is_correct: False, num_tokens: 2754, valid_response_length: 4096delta_score: -0.7635, reward_response.is_correct: False, num_tokens: 3177, valid_response_length: 632delta_score: -0.9332999999999999, reward_response.is_correct: False, num_tokens: 3860, valid_response_length: 749



delta_score: -0.7635, reward_response.is_correct: False, num_tokens: 3177, valid_response_length: 632
delta_score: -0.40259999999999996, reward_response.is_correct: False, num_tokens: 2754, valid_response_length: 4096




reward_tensor's shape : torch.Size([16, 4096])
len reward_extra_infos_dict['reward']: 48
test_gen_batch meta info: {'eos_token_id': 151645, 'pad_token_id': 151643, 'recompute_log_prob': False, 'do_sample': False, 'validate': True, 'global_steps': 0}
validation generation end
delta_score: -0.9251999999999999, reward_response.is_correct: False, num_tokens: 3499, valid_response_length: 415delta_score: -0.9251999999999999, reward_response.is_correct: False, num_tokens: 3499, valid_response_length: 415delta_score: -0.72, reward_response.is_correct: False, num_tokens: 3060, valid_response_length: 660delta_score: -0.07050000000000001, reward_response.is_correct: False, num_tokens: 2357, valid_response_length: 2122delta_score: -0.6927, reward_response.is_correct: False, num_tokens: 3060, valid_response_length: 751

delta_score: -0.07050000000000001, reward_response.is_correct: False, num_tokens: 2357, valid_response_length: 2122
delta_score: -0.7836, reward_response.is_correct: False, num_tokens: 1484, valid_response_length: 4096delta_score: -0.7836, reward_response.is_correct: False, num_tokens: 1484, valid_response_length: 4096

delta_score: -0.02310000000000001, reward_response.is_correct: False, num_tokens: 679, valid_response_length: 756delta_score: -0.02310000000000001, reward_response.is_correct: False, num_tokens: 648, valid_response_length: 571delta_score: -0.017100000000000004, reward_response.is_correct: False, num_tokens: 679, valid_response_length: 622


delta_score: -0.02310000000000001, reward_response.is_correct: False, num_tokens: 648, valid_response_length: 571



reward_tensor's shape : torch.Size([12, 4096])
len reward_extra_infos_dict['reward']: 60
("Initial validation metrics: {'val-core//reward/mean@2': -0.5160750003842016, "
 "'val-aux//reward/std@2': 0.0005549999885261058, "
 "'val-core//reward/best@2/mean': -0.5157930603900304, "
 "'val-core//reward/best@2/std': 0.0004780531633099885, "
 "'val-aux//reward/worst@2/mean': -0.5163747003780057, "
 "'val-aux//reward/worst@2/std': 0.00046712407450034277}")
step:0 - val-core//reward/mean@2:-0.5160750003842016 - val-aux//reward/std@2:0.0005549999885261058 - val-core//reward/best@2/mean:-0.5157930603900304 - val-core//reward/best@2/std:0.0004780531633099885 - val-aux//reward/worst@2/mean:-0.5163747003780057 - val-aux//reward/worst@2/std:0.00046712407450034277
Training Progress:   0%|          | 0/15114 [00:00<?, ?it/s]Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::WorkerDict.actor_rollout_update_actor()[39m (pid=793252, ip=172.28.177.94, actor_id=2e312706822984c52a15dbaa04000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7eeff15b7b60>)
delta_score: -0.05369999999999997, reward_response.is_correct: False, num_tokens: 745, valid_response_length: 924delta_score: -0.06779999999999997, reward_response.is_correct: False, num_tokens: 425, valid_response_length: 651

delta_score: -1.0362, reward_response.is_correct: False, num_tokens: 3906, valid_response_length: 452delta_score: -1.1582999999999999, reward_response.is_correct: False, num_tokens: 235, valid_response_length: 4096
delta_score: -1.0052999999999999, reward_response.is_correct: False, num_tokens: 745, valid_response_length: 4096
delta_score: -0.21960000000000002, reward_response.is_correct: False, num_tokens: 104, valid_response_length: 836delta_score: -0.6555, reward_response.is_correct: False, num_tokens: 3059, valid_response_length: 874
delta_score: -0.11009999999999998, reward_response.is_correct: False, num_tokens: 104, valid_response_length: 471delta_score: -0.48539999999999994, reward_response.is_correct: False, num_tokens: 2347, valid_response_length: 729delta_score: -0.12329999999999997, reward_response.is_correct: False, num_tokens: 1669, valid_response_length: 1258delta_score: -0.0645, reward_response.is_correct: False, num_tokens: 425, valid_response_length: 640
delta_score: -0.6657, reward_response.is_correct: False, num_tokens: 3059, valid_response_length: 840delta_score: -0.40169999999999995, reward_response.is_correct: False, num_tokens: 235, valid_response_length: 1574
delta_score: -0.06240000000000001, reward_response.is_correct: False, num_tokens: 1669, valid_response_length: 1877
delta_score: -1.0128, reward_response.is_correct: False, num_tokens: 3906, valid_response_length: 530

delta_score: -0.39959999999999996, reward_response.is_correct: False, num_tokens: 2347, valid_response_length: 1015





reward_tensor's shape : torch.Size([16, 4096])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/yiran/miniconda3/envs/l1-yaoqi/lib/python3.12/site-packages/verl/single_controller/ray/base.py", line 720, in func
    return getattr(self.worker_dict[key], name)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/yiran/miniconda3/envs/l1-yaoqi/lib/python3.12/site-packages/verl/single_controller/base/decorator.py", line 430, in inner
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/yiran/miniconda3/envs/l1-yaoqi/lib/python3.12/site-packages/verl/workers/fsdp_workers.py", line 735, in update_actor
    metrics = self.actor.update_policy(data=data)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/yiran/miniconda3/envs/l1-yaoqi/lib/python3.12/site-packages/verl/utils/profiler/performance.py", line 105, in f
    return self.log(decorated_function, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/yiran/miniconda3/envs/l1-yaoqi/lib/python3.12/site-packages/verl/utils/profiler/performance.py", line 118, in log
    output = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/yiran/miniconda3/envs/l1-yaoqi/lib/python3.12/site-packages/verl/workers/actor/dp_actor.py", line 466, in update_policy
    loss.backward()
  File "/home/yiran/miniconda3/envs/l1-yaoqi/lib/python3.12/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/home/yiran/miniconda3/envs/l1-yaoqi/lib/python3.12/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/home/yiran/miniconda3/envs/l1-yaoqi/lib/python3.12/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 9.27 GiB. GPU 0 has a total capacity of 79.25 GiB of which 2.06 GiB is free. Including non-PyTorch memory, this process has 77.15 GiB memory in use. Of the allocated memory 75.39 GiB is allocated by PyTorch, with 86.00 MiB allocated in private pools (e.g., CUDA Graphs), and 108.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::WorkerDict.actor_rollout_update_actor()[39m (pid=793250, ip=172.28.177.94, actor_id=a2775f2099801d9764cbe4bc04000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7f71265ffb00>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/yiran/miniconda3/envs/l1-yaoqi/lib/python3.12/site-packages/verl/single_controller/ray/base.py", line 720, in func
    return getattr(self.worker_dict[key], name)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/yiran/miniconda3/envs/l1-yaoqi/lib/python3.12/site-packages/verl/single_controller/base/decorator.py", line 430, in inner
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/yiran/miniconda3/envs/l1-yaoqi/lib/python3.12/site-packages/verl/workers/fsdp_workers.py", line 735, in update_actor
    metrics = self.actor.update_policy(data=data)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/yiran/miniconda3/envs/l1-yaoqi/lib/python3.12/site-packages/verl/utils/profiler/performance.py", line 105, in f
    return self.log(decorated_function, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/yiran/miniconda3/envs/l1-yaoqi/lib/python3.12/site-packages/verl/utils/profiler/performance.py", line 118, in log
    output = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/yiran/miniconda3/envs/l1-yaoqi/lib/python3.12/site-packages/verl/workers/actor/dp_actor.py", line 466, in update_policy
    loss.backward()
  File "/home/yiran/miniconda3/envs/l1-yaoqi/lib/python3.12/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/home/yiran/miniconda3/envs/l1-yaoqi/lib/python3.12/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/home/yiran/miniconda3/envs/l1-yaoqi/lib/python3.12/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 9.27 GiB. GPU 0 has a total capacity of 79.25 GiB of which 2.08 GiB is free. Including non-PyTorch memory, this process has 77.13 GiB memory in use. Of the allocated memory 75.39 GiB is allocated by PyTorch, with 86.00 MiB allocated in private pools (e.g., CUDA Graphs), and 88.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::WorkerDict.actor_rollout_update_actor()[39m (pid=793251, ip=172.28.177.94, actor_id=94dc7852719658ecb0c68f0904000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7f99258480e0>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/yiran/miniconda3/envs/l1-yaoqi/lib/python3.12/site-packages/verl/single_controller/ray/base.py", line 720, in func
    return getattr(self.worker_dict[key], name)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/yiran/miniconda3/envs/l1-yaoqi/lib/python3.12/site-packages/verl/single_controller/base/decorator.py", line 430, in inner
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/yiran/miniconda3/envs/l1-yaoqi/lib/python3.12/site-packages/verl/workers/fsdp_workers.py", line 735, in update_actor
    metrics = self.actor.update_policy(data=data)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/yiran/miniconda3/envs/l1-yaoqi/lib/python3.12/site-packages/verl/utils/profiler/performance.py", line 105, in f
    return self.log(decorated_function, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/yiran/miniconda3/envs/l1-yaoqi/lib/python3.12/site-packages/verl/utils/profiler/performance.py", line 118, in log
    output = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/yiran/miniconda3/envs/l1-yaoqi/lib/python3.12/site-packages/verl/workers/actor/dp_actor.py", line 466, in update_policy
    loss.backward()
  File "/home/yiran/miniconda3/envs/l1-yaoqi/lib/python3.12/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/home/yiran/miniconda3/envs/l1-yaoqi/lib/python3.12/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/home/yiran/miniconda3/envs/l1-yaoqi/lib/python3.12/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 9.27 GiB. GPU 0 has a total capacity of 79.25 GiB of which 2.06 GiB is free. Including non-PyTorch memory, this process has 77.15 GiB memory in use. Of the allocated memory 75.39 GiB is allocated by PyTorch, with 86.00 MiB allocated in private pools (e.g., CUDA Graphs), and 108.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
